import ollama
from flask import Flask , request , render_template
import os
import json
import numpy as np
from numpy.linalg import norm
import logging

# Create Flask app
app = Flask(__name__)



# open a file and return paragraphs
def parse_file(filename):
    with open(filename, encoding="utf-8-sig") as f:
        paragraphs = []
        buffer = []
        for line in f.readlines():
            line = line.strip()
            if line:
                buffer.append(line)
            elif len(buffer):
                paragraphs.append((" ").join(buffer))
                buffer = []
        if len(buffer):
            paragraphs.append((" ").join(buffer))
        return paragraphs


def save_embeddings(filename, embeddings):
    # create dir if it doesn't exist
    if not os.path.exists("embeddings"):
        os.makedirs("embeddings")
    # dump embeddings to json
    with open(f"embeddings/{filename}.json", "w") as f:
        json.dump(embeddings, f)


def load_embeddings(filename):
    # check if file exists
    if not os.path.exists(f"embeddings/{filename}.json"):
        return False
    # load embeddings from json
    with open(f"embeddings/{filename}.json", "r") as f:
        return json.load(f)


def get_embeddings(filename, modelname, chunks):
    # check if embeddings are already saved
    if (embeddings := load_embeddings(filename)) is not False:
        return embeddings
    # get embeddings from ollama
    embeddings = [
        ollama.embeddings(model=modelname, prompt=chunk)["embedding"]
        for chunk in chunks
    ]
    # save embeddings
    save_embeddings(filename, embeddings)
    return embeddings


# find cosine similarity of every chunk to a given embedding
def find_most_similar(needle, haystack):
    needle_norm = norm(needle)
    similarity_scores = [
        np.dot(needle, item) / (needle_norm * norm(item)) for item in haystack
    ]
    return sorted(zip(similarity_scores, range(len(haystack))), reverse=True)

@app.route('/', methods=['GET', 'POST'])
def main():

    query_input = None
    response = None
    
    SYSTEM_PROMPT = """If you're unsure, just say that you don't know.
        The text or context that you have been provided is called as the 'fitness database'.
        Context:
    """
    # # open file
    filename = "Ultimate-Bodybuilding-Manual-IAFS.txt"
    paragraphs = parse_file(filename)

    embeddings = get_embeddings(filename, "nomic-embed-text", paragraphs)
    if request.method == 'POST':
        query_input = request.form.get('query-input')

        # prompt = input("what do you want to know? -> ")
        # strongly recommended that all embeddings are generated by the same model (don't mix and match)
        prompt_embedding = ollama.embeddings(model="nomic-embed-text", prompt=query_input)["embedding"]
        # find most similar to each other
        most_similar_chunks = find_most_similar(prompt_embedding, embeddings)[:5]

        
        if query_input:
            try:
                response = ollama.chat(
                model="llama3",
                messages=[
                    {
                        "role": "system",
                        "content": SYSTEM_PROMPT
                        + "\n".join(paragraphs[item[1]] for item in most_similar_chunks),
                    },
                    {"role": "user", "content": query_input},
                ],
            )
            except Exception as e:
                logging.error(f"Error during chatbot invocation: {e}")
                output = "Sorry, an error occurred while processing your request."
    return render_template('index.html', query_input=query_input, output=response)
        # print("\n\n")
        # print(response["message"]["content"])


if __name__ == "__main__":
    app.run(debug=True)
